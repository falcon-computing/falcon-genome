#!/usr/bin/env python
import argparse
import json
import csv
import fileinput as fin
import re
import StringIO
from fcs_manager import QueueManager
from subprocess import call

# argument settings
parser = argparse.ArgumentParser(description='Compute handler')

parser.add_argument('-i', '--log-path', dest='log_path', nargs='?',
                     default='', help='path to logging message')

parser.add_argument('-d', '--root-dir', dest='root_dir', nargs='?',
                     default='./', help='root dir')

parser.add_argument('--in_queue', dest='in_queue', nargs='?',
                     type=int, default=1030, help='id of the demux queue')

args = parser.parse_args()

class ComputeManager(QueueManager):
  def worker_func(self, msg):
    data = json.loads(msg)
    self.logger.info('run_id: ' + data['run_id'] + ' ' +
                'demux_dir: '+ data['demux_dir'])

    # start parsing sample sheet
    samplesheet_path = data['demux_dir'] + '/SampleSheet.csv'
    contents = ''

    try: 
      with open(samplesheet_path, 'r') as ssheet_file:
        found_table = 0
        for line in ssheet_file:
          if '[data]' in line.lower():
            found_table = 1
          elif re.search('\[[A-Za-z].*\]', line):
            found_table = 0
          else:
            if found_table:
              contents = contents + line

      csv_file = StringIO.StringIO(contents)
      reader = csv.DictReader(csv_file)

      successful = True

      # current marking place for analysis pipeline
      row_name = 'Description'

      # make sure we only process one sample once
      completed_samples = {}

      for row in reader:
        if row[row_name]:
          pipeline = row[row_name].lower()
          sample_id = row['Sample_ID']
          project_id = row['Sample_Project']

          # unique id for a sample in this run
          sample_key = project_id + '-' + sample_id;
          if sample_key in completed_samples:
            self.logger.info("Duplicated processing found for sample "+ sample_id)
            continue

          fastq_dir = data['demux_dir'] +'/'+ project_id
      
          self.logger.info('start a compute task for run: '+ data['run_id']
                          +', sample: '+ sample_id)

          ret = call([self.dir + '/pipelines/'+ pipeline +'.sh',
                  sample_id,
                  fastq_dir,
                  args.root_dir +'/'+ data['run_id'] +'/'+ project_id])
  
          # check the return signal 
          if ret == 0:
            self.logger.info('finished processing sample '+ sample_id)
            completed_samples[sample_key] = 'done'
          else:
            self.logger.info('failed to process sample '+ sample_id)
            successful = False

      if successful: 
        self.logger.info('run '+ data['run_id'] +' is sucessful, cleaning up')
        # TODO: clean up
        call(['rm', '-rf', fastq_dir])
      else:
        raise ValueError('Compute pipeline failed')

    except:
      self.logger.error('run '+ data['run_id'] +' failed')
      self.checkpoint(data['run_id']+'.json', msg)

q = ComputeManager(name = "Compute Manager",
      log_path = args.log_path,
      qin_id = args.in_queue,
      num_workers = 1)
q.listen()
